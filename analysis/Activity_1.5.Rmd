### Activity 1.5: Statistical and graphical analyses ###
========================================================
  
  
#### Activity 1.5.1: A simple linear regression model
  
  
Let's now consider fitting a simple linear regression model
to some car data that compares stopping distance to speed
for 50 vehicles.

Because these data are available in base R, we don't have to
load the data or read it from a file.  We can simply access
it by typing 'cars'.

Look at the structure of the cars data.

```{r}
str(cars)
```

Now that we know what the column names are, let's plot the
data:

```{r}
plot(cars[,"speed"], cars[,"dist"],
xlab = "Speed", ylab = "Distance")
```

Now let's fit a simple linear regression model, to use
speed to predict the stopping distance.  We use the linear
model function, `lm()`.

```{r}
slrModel <- lm(dist ~ speed, data = cars)
```

If we call `str()` on `slrModel`, we see that it is a
complicated R object called a list.

```{r}
str(slrModel)
```

Fortunately, there are a variety of method functions we
can use to extract information from the `slrModel` object.
For example: a call to `summary()` on the `slmModel` object
gives the slope, intercept, R-squared, and other statistics
about the model.

```{r}
summary(slrModel)
```

We can also extract the coefficients of the model.

```{r}
coef(slrModel)
```

And we can superimpose the regression line on the plot
(remake the plot of these data first before executing this
command if you closed the plot).

```{r}
slrModel <- lm(cars[,"dist"]~ cars[,"speed"])
plot(cars[,"speed"], cars[,"dist"], xlab='Speed', ylab='Distance')
abline(a=-17.57, b= 3.93, col = "Blue")
```

We can also add a smoothed line that follow the data
using `lowess()`.

```{r}
slrModel <- lm(cars[,"dist"]~ cars[,"speed"])
plot(cars[,"speed"], cars[,"dist"], xlab='Speed', ylab='Distance')
abline(a=-17.57, b= 3.93, col = "Blue")
lines(lowess(cars$speed, cars$dist), col = "Red")
```

You can close the current plot with:

```{r}
dev.off()
```

Alternatively, you can close all open plots with:

```{r}
graphics.off()
```

#### Exercise 1.5.1: A simple linear regression model

1) Look at the Old Faithful geyser data in Yellowstone
National Park.  It's the `faithful` dataset available in
the base R distribution.  Use `help(faithful)` or `?faithful`
to see its documentation

2) Plot the waiting time to the next eruptions vs. the
duration of the eruption

3) Fit a simple linear regression model to predict waiting
time from the eruption duration

4) Overlay the fitted model on the plot with a red line


#### Activity 1.5.2: Factors


"Factors" is a technique R uses to efficiently encode
categorical variables.  Factors are primarily used in
statistical models. Consider that a categorical predictor
variable, with n distinct possible values, must be
represented by n-1 dummy (or indicator) variables in the
model matrix. The factor representation tells R to
automatically create the dummy variables that are used in,
e.g., Analyis of Variance (ANOVA) and Analysis of Covariance
(ANCOVA) models.  Factors are also used for plotting data
in groups.

Suppose we had a categorical variable with three possible
values: "animal", "plant", or "non-living".  We'll use
sample() to randomly order the character vector.

```{r}
var <- sample(c(rep("animal", 3), rep("plant", 2),
                rep("non-living", 4)))
var
```

We could create a factor vector as follows:
  
  ```{r}
f1 <- factor(var)
f1
```

Notice in the structure, we see levels and the numeric
encodings of the categorical values.

```{r}
str(f1)
```

Factors have a nice `summary()` method that counts the number
of elements that occur in each level.

```{r}
summary(f1)
```

We can extract the levels from the factor vector.  Note how
they appear in alphabetical order by default.

```{r}
levels(f1)
```

In this case, this means that "animal" is assigned a value of
1, "non-living" a value of 2, and "plant" a value of 3.  We
can see the numerical encodings by using `as.numeric()`.

```{r}
as.numeric(f1)
```

And we can pair the codings side-by-side in a data frame
with the original character vector to see the mapping.

```{r}
data.frame(original = var, numerical.coding = as.numeric(f1))
```

Suppose we prefer a different mapping: plant = 1, animal = 2,
and non-living = 3. This will do the trick:
  
```{r}
f2 <- factor(var, levels = c("plant", "animal", "non-living"))
```

And we can see the new mapping:
  
```{r}
data.frame(original = var, numerical.coding = as.numeric(f2))
```

We subset factors by referencing their level names, not their
numerical coding.  For example if we wanted to identify the
elements that were "plants", we could do the following:
  
```{r}
f2 == "plant"
```

Or, to get the index numbers:
  
```{r}
which(f2 == "plant")
```

If the original data that were used to create a factor are
character, converting the factor back to a character vector
is straightforward using `as.character()`.

```{r}
varRestored1 <- as.character(f1)
identical(var, varRestored1)
```

And this back-conversion works regardles of how the levels
were specified.

```{r}
varRestored2 <- as.character(f2)
identical(var, varRestored2)
```

We can also create factors from numeric variables as well
(provided the variable has a limited number of distinct
 values).

```{r}
numVar <- rep(4:1, each = 2)
numVar
f3 <- factor(numVar)
f3
```

If the original data that were used to create a factor are
numeric, we have to take some special steps to properly
convert the factor back to its original integer (or numeric)
values.

```{r}
numVarRestored <- as.integer(levels(f3)[f3])
identical(numVarRestored, numVar)
```

And we can even give levels new names, or labels.
Suppose we want 1 = "good", 2 = "bad", 3 = "ugly", and
4 = "obnoxious".

```{r}
f4 <- factor(numVar, levels = 1:4,
             labels = c("good", "bad", "ugly", "obnoxious"))
```

Let's check it out in a variety of ways:

```{r}
f4
str(f4)
data.frame(original = numVar, numerical.coding = as.numeric(f4),
labels = as.character(f4))
```

We can also create a new mapping:  2 = "good", 1 = "bad",
4 = "ugly", and 3 = "obnoxious".

```{r}
f5 <- factor(numVar, levels = c(2, 1, 4, 3),
labels = c("good", "bad", "ugly", "obnoxious"))
f5
data.frame(original = numVar, 
            numerical.coding = as.numeric(f5),
            labels = as.character(f5)
          )
```

As before, if we want to reference elements of the factor, for
the purpose of subsetting, we have to use the level labels:

```{r}
subsetLogical <- f5 == "good"
subsetIndexes <- which(f5 %in% c("good", "ugly"))
```

And we can then subset the factor accordingly:

```{r}
f5[subsetLogical]
f5[subsetIndexes]
```

Let's see how levels work in practice with ANOVA. Let's
first cook up some fake data from the standard normal
distribution to go with our numerical factor.

```{r}
x <- rnorm(length(numVar))
```

And then we'll look at the coefficients of an ANOVA model and
how R labels them based on the factor levels.  First, we
fit the ANOVA model:
  
```{r}
m1 <- lm(x ~ f3)
```

Notice how the parameter (coefficient) names of the ANOVA
model are `f3` with the levels of `f3` appended to them
(except for the first level, which is captured in the
intercept).

```{r}
coef(m1)
levels(f3)
```

And here we have `f4` with the levels of `f4` appended:

```{r}
coef(lm(x ~ f4))
levels(f4)
```


#### Exercise 1.5.2: Factors

1) Create a numeric vector with arbitrarily repeated
values of 1.2, 3, and 7.9 .

2) Create the numeric vector to a factor, where 3 will be
numerically encoded as a 1, 7.9 as a 2, and 1.2 as 3.
Display the numerical encoding

3) Display the levels of the factor

4) Convert the factor back to the original numerical vector

5) Create a subset of the factor when it equals 1.2 or 3



#### Activity 1.5.3: Trellis plots

This example introduces making trellis (or lattice) plots
using the `lattice` package and the `mtcars` dataset.

Load the lattice package (for making trellis plots).

```{r}
library(lattice)
```

For convenience, attaching the data frame places it in the
search path so that we don't always have to reference `mtcars`
each time we want to extract a column from the data.

```{r}
attach(mtcars)
```

Now that we've attached it, notice how it appears in the
2nd position of R's search path?

```{r}
search()
```

Since `search()` returns a character vector, we could use the
following to verify `mtcars` is in the search path:

```{r}
"mtcars" %in% search()
```

And thus, instead of using either of these:

```{r}
head(mtcars$mpg)
head(mtcars[,"mpg"])
```

We can simply use `mpg`

```{r}
head(mpg)
```

In preparation for the lattice plots, we create factors with
value labels that will aid in annotation and in dividing the
data into groups based on the number of gears and the
number of cylinders.

```{r}
gear.f <- factor(gear, levels = c(3, 4, 5),
labels = c("3gears", "4gears", "5gears"))

cyl.f <- factor(cyl, levels = c(4, 6, 8),
labels = c("4cyl", "6cyl", "8cyl"))
```

Let's make a kernel density plot of mpg for all the vehicles.
A kernel density is a smoothed plot of the raw
data. Notice that `densityplot()` comes from the `lattice`
package.

```{r}
densityplot(~mpg, main = "Density Plot",
            xlab = "Miles per Gallon")
```

Kernel density plots for each cylinder type.  Notice
this is where we use the factor variable for cylinder.

```{r}
densityplot(~mpg | cyl.f,
            main = "Density Plot by Number of Cylinders",
            xlab = "Miles per Gallon")
```

Scatterplots of mpg vs. weight for each combination of
cylinder type and gear type.  `xyplot()` is another function
from the `lattice` package.

```{r}
xyplot(mpg ~ wt | cyl.f * gear.f,
       main = "Scatterplots by Cylinders and Gears",
       ylab = "Miles per Gallon", xlab = "Car Weight")
```

Scatterplot matrix over a number of the columns:

```{r}
splom(mtcars[,c(1, 3:6)], main = "MTCARS Data")
```

Now that we're through with `mtcars`, we detach it from the
search path.

```{r}
detach(mtcars)
```

It's no longer there:

```{r}
"mtcars" %in% search()
```

Close the graphics window(s).

```{r}
graphics.off()
```

#### Exercise 1.5.3: Trellis plots

1) Attach the quakes data that comes with R. Look at the
columns using `head()` or `str()`

2) Use xyplot to plot lat vs. long for the location of the
earthquakes. Label the axes and the main graphs
using the following command to bin the depth variable into
9 bins:
  
  `depthbin <- equal.count(quakes$depth, number = 9,
                          overlap = 0)`

Make sure you have loaded the lattice package via:
  `library(lattice)`

3) Now use xyplot() to plot lat,long for each depth bin of
the earthquake to create a set of graphs in a trellis
display.



#### Activity 1.5.4: Time series analysis

The data for this example contain daily counts of the number
(`Freq`) of domestic airline flights in the United States
from 1999 until the spring of 2008

Let's load the data, which we previously saved as an
Rdata object.  Wrapping `print()` around the call to `load()`
will list the name(s) of the R objects that were loaded from
the Rdata file

```{r}
print(load("dailycount.Rdata"))
```

R has a built-in function for time series decompostion, where
seasonal, trend, and irregular components can be separated
from one another.  `stl()` is the function that does this.

Use R's built-in `stl()` function:

```{r}
daily.stl <- stl(ts(dailycount$Freq / 1000, frequency = 7),
                 s.window = 51, s.degree = 1, t.window = 19)
```

And a plot shows a visualization of the decomposition:

```{r}
plot(daily.stl)
```

To explore some additional capability for time series
decomposition that was developed by Ryan Hafen, we need to
download and install his package from github.  To do that,
we need a helper function, install_github(), which is
available in the devtools package that is available from CRAN.

Since the `devtools`  and `stl` packages are already
installed on the AWS cluster, we will skip these 3 steps
Otherwise, you would need to run these 3 commands:

```{r eval=FALSE}
install.packages("devtools")
library(devtools)
install_github("stl2", "hafen")
```

Load the `stl2` package.

```{r}
library(stl2)
```

Define a character vector of weekdays:

```{r}
weekdays <- c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")
```

And now we calculate the time series decomposition.
(To learn more about this, begin with `?stl2`.)

```{r}
daily.stl2 <- stl2(dailycount$Freq / 1000, n.p = 7,
t = dailycount$date, s.window = 51,
s.degree = 1, t.window = 19,
sub.labels = weekdays, sub.start = 3)
```

Plot the results of the decomposition:

```{r}
plot(daily.stl2, ylab = "Daily Flights (thousands)",
xlab = "Time")
```

We have decomposed the time series into seasonal, trend,
and remainder components.  The trend component captures
the overall level over time, the seasonal component (here
day-of-week) captures changes across week over time,
and the remainder is the residual noise not explained by
the time series model.

When we fit models to data, we should look at those points
where the model does not fit well.  Hence we study
abnormal outliers in remainder:

```{r}
plot.seasonal(daily.stl2, layout = c(7, 1))
```

We see that Wednesday is declining over the years, while
Monday and Tuesday are increasing.  No wonder flights
are cheaper on Wednesdays! This pattern would have been
difficult to see without the combination of the
time series model and the lattice plot visualization.

Now we turn to the trend portion of the decomposition:
We see a drop on 9/11/2001, then a huge jump back up in 2003.

```{r}
plot.trend(daily.stl2)
```

This plot gives us a sense of the magnitude of the
residuals (remainder), broken down by day of the week,
adjusting for the seasonal effect:

```{r}
plot.cycle(daily.stl2)
```

Same residual plot, for each day of the week.

```{r}
plot.rembycycle(daily.stl2)
```

This leads us to look for places where the residuals
are negative--indicating the model is underpredicting
the number of flights on those days.

What dates in remainder are large in magnitude?

```{r}
ind <- which(remainder(daily.stl2) < -3)
dailycount[ind,]
```

Thanksgiving, New Years, 4th of July, Christmas...

And if we look for the largest remainder?

```{r}
ind <- which(remainder(daily.stl2) < -4)
dailycount[ind,]
```

We see Thanksgiving is the worst.
